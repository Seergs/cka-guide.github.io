{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gu\u00eda CKA - Kubernetes Administrator","text":"<p>Esta gu\u00eda se arm\u00f3 para ayudarnos a estudiar y tener todo organizado en un solo lugar.</p> <p>La gu\u00eda est\u00e1 organizada por temas y contiene:</p> <ul> <li>Apuntes directos y pr\u00e1cticos</li> <li>Ejemplos de comandos que se pueden necesitar</li> </ul>"},{"location":"2025-updates/","title":"2025 Updates","text":""},{"location":"2025-updates/#admission-controllers","title":"Admission Controllers","text":"<p>Son plugins para configurar reglas antes de la creacion de un pod o restricciones.  Kubernetes viene con varios plugins, pero tu tambi\u00e9n puedes crear los tuyos, por ejemplo uno que se asegure que no se corra un container como root.</p> <p>Si el api server est\u00e1 corriendo como pod se puede hacer esto para ver los plugins</p> <pre><code>ps -ef | grep kube-apiserver | grep admission-plugins\n</code></pre>"},{"location":"2025-updates/#validating-admission-controller","title":"Validating Admission Controller","text":"<p>Se usan mas que nada para validar una accion</p> <p>Un ejemplo de este es el: NamespaceLifecycle, que validate que exista el namespace y evita que se borren los 3 NS por defecto: default, kube-system, kube-public.</p>"},{"location":"2025-updates/#mutating-admission-controller","title":"Mutating Admission Controller","text":"<p>Mutan los recursos en base a alguna validaci\u00f3n. Estos corren antes de los validation por si los validation deben checar algo ya este actualizado.</p> <p>Un ejemplo de este es el: DefaultStorageClass que le pone un storage class por defecto a los PVC cuando no tienen uno especificado.</p>"},{"location":"2025-updates/#hpa-horizontal-pod-autoscaler","title":"HPA (Horizontal Pod Autoscaler)","text":"<p>Utiliza el metric-server para obtener metricas de los pods</p> <p>Si te aparece como <code>unknown</code> es porque hpa no puede obtener las metricas y puede ser porque no esta definido en el deployment los cpu request and limits</p> <p> </p>"},{"location":"2025-updates/#vpa-vertical-pod-autoscaler","title":"VPA (Vertical Pod Autoscaler)","text":""},{"location":"2025-updates/#instalar","title":"Instalar","text":"<ul> <li>Step 1: Install VPA Custom Resource Definitions (CRDs)</li> <li>Step 2: Install VPA Role-Based Access Control (RBAC)</li> </ul> <p>Ejemplo</p> <pre><code>apiVersion: \"autoscaling.k8s.io/v1\"\nkind: VerticalPodAutoscaler\nmetadata:\n  name: flask-app\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: flask-app-4\n  updatePolicy:\n    updateMode: \"Off\"  # You can set this to \"Auto\" if you want automatic updates\n  resourcePolicy:\n    containerPolicies:\n      - containerName: '*'\n        minAllowed:\n          cpu: 100m\n        maxAllowed:\n          cpu: 1000m\n        controlledResources: [\"cpu\"]\n</code></pre> <p>Explicaci\u00f3n de que significa cada parte de cuando haces un describe</p> <ul> <li><code>containerName</code>: flask-app-4: La recomendaci\u00f3n se aplica al contenedor llamado flask-app-4.</li> <li><code>lowerBound (cpu: 100m)</code>: Este es el valor m\u00ednimo de CPU que el contenedor puede utilizar. En este caso, es 100 milicores de CPU (100m), lo que equivale al 10% de una CPU completa.</li> <li><code>target (cpu: 296m)</code>: Este es el valor recomendado de uso de CPU. El contenedor debe operar en promedio utilizando 296 milicores de CPU (0,296 CPU).</li> <li><code>uncappedTarget (cpu: 296m)</code>: Esto indica que no hay un l\u00edmite superior expl\u00edcito para el valor recomendado de CPU, es decir, el contenedor puede usar hasta este valor de forma flexible sin restricciones.</li> <li><code>upperBound (cpu: \u201c1\u201d)</code>: Este es el valor m\u00e1ximo recomendado de CPU para el contenedor, lo que significa que el contenedor puede usar hasta 1 CPU completa (1000m).</li> </ul> <pre><code>status:\n  conditions:\n  - lastTransitionTime: \"2025-02-27T21:03:20Z\"\n    status: \"True\"\n    type: RecommendationProvided\n  recommendation:\n    containerRecommendations:\n    - containerName: flask-app-4\n      lowerBound:\n        cpu: 100m\n      target:\n        cpu: 296m\n      uncappedTarget:\n        cpu: 296m\n      upperBound:\n        cpu: \"1\"\n</code></pre>"},{"location":"2025-updates/#vpa-vs-hpa","title":"VPA vs HPA","text":""},{"location":"annotations-and-labels/","title":"Annotations & Labels","text":""},{"location":"annotations-and-labels/#labels","title":"Labels","text":"<p>Se usan para conectar diferentes objetos entre s\u00ed, por ejemplo Pods, Services, etc.</p> <p></p>"},{"location":"annotations-and-labels/#annotation","title":"Annotation","text":"<p>Solo son de prop\u00f3sito informativo.</p>"},{"location":"architecture/","title":"Cluster Architecture","text":"<ul> <li>Un container engine (agente) debe estar instalado en todos los nodos (por ejemplo Docker, containerd o Rocket)</li> </ul>"},{"location":"architecture/#worker-nodes","title":"Worker Nodes","text":"<p>Trackea y monitorea los containers. Sirven para distribuir la carga.</p>"},{"location":"architecture/#kubelet","title":"Kubelet","text":"<p>Corre en cada nodo del cluster y espera instrucciones del kubeapi server y crea o destruye containers. Primero el kubeapi server le pregunta al scheduler en que nodo poner cierto pod, y despu\u00e9s el kubeapi server le dice al kubelet para crearlo. Este lo crea, descargando la imagen y creando el container. No viene instalado por defecto con kubeadmn, tienes que instalarlo a mano en cada nodo.</p> <p>Contiene cAdvisor que se encarga de recolectar metricas del nodo y los pods.</p>"},{"location":"architecture/#kube-proxy","title":"Kube proxy","text":"<p>Se asegura que haya reglas necesarias en los worker nodes para permitir que los containers que est\u00e1n corriendo puedan alcanzar otros en otro worker node. Configura por ejemplo iptables en cada nodo, para poder \"configurar\" los kubernetes service y los containers se puedan conectar entre nodos.</p>"},{"location":"architecture/#master-node","title":"Master Node","text":"<p>Responsable del manejo del cluster y guarda informaci\u00f3n sobre los worker nodes, qu\u00e9 container va en qu\u00e9 worker node. Tiene el control plane. Contiene etcd, que es una DB (key-valu) que guarda informaci\u00f3n sobre todo: nodos, pods, secrets, configmaps, etc. Cuando utilizas kubectl est\u00e1s interactuando por debajo con etcd.</p>"},{"location":"architecture/#kube-scheduler","title":"kube scheduler","text":"<p>Identifica en qu\u00e9 worker node se pondr\u00e1 un container en base a la capacidad de los nodos y al tama\u00f1o del conatiner a crear.  Se basa en filtrar los nodos en base a los requerimientos, por ejemplo filtra por resources y despu\u00e9s los prioriza en base a un ranking.</p>"},{"location":"architecture/#controller-manager","title":"Controller Manager","text":"<p>Contiene adentro varios controllers: replication controller, node-controller, etc. Viene instalado por defecto con todos, pero se puede configurar al gusto, como quitar alguno o configurar alg\u00fan parametro. Node controller: nodos, hacer onboarding de meter nuevos nodos al cluster, manejar cuando un nodo ya no funciona o se borra Replication controller: Se encarga de decidir el n\u00famero de containers corriendo.</p>"},{"location":"architecture/#kube-api","title":"Kube Api","text":"<p>Primary management component, orquesta todas las operaciones dentro del cluster:</p> <ul> <li>Autentifica al usuario</li> <li>Valida la request</li> <li>Obtiene informaci\u00f3n</li> <li>Actualiza el etcd</li> <li>Llama al scheduler para saber qu\u00e9 nodo usar</li> <li>Llama al agente para que cree el pod en el worker node</li> </ul> <p>Container Runtime Interface (CRI): Permite a cualquier agente para funcionar como el container runtime de k8s</p> <ul> <li>ctr: Correr containers con containerd por separado</li> <li>nerdtcl: Es mejorcito</li> <li>crictl: serve para cualquier container runtime que es el CRI compatible (como rocket)</li> </ul> <p>Kubectl - El kubectl interactua con el kube api server</p>"},{"location":"cluster-maintenance/","title":"Cluster Maintenance","text":"<ul> <li>Si un nodo est\u00e1 caido por mas de 5 minutos, los pods morir\u00e1n, pero si existen replicas se crear\u00e1n los pods que se murieron en algun otro nodo disponible. Si no hay replicas se morir\u00e1 para siempre.</li> <li>Para mover los pods a otro nodo se utiliza este comando: <code>kubectl drain node-1</code>. Esto tambi\u00e9n lo marca como Unschedulable</li> <li>Cuando el nodo regrese en linea, se require correr el <code>kubectl uncordon node-1</code></li> <li>Si corres solamente el <code>kubectl cordon node-1</code> esto unicamente lo marca como Unschedulable pero no mueve pods a otro nodo.</li> </ul>"},{"location":"cluster-maintenance/#k8s-software-versions","title":"K8s Software Versions","text":"<p>Existen tambi\u00e9n versiones alpha y beta. Ejemplo:</p> <ul> <li><code>V1.10.0-alpha</code>: Estas versiones est\u00e1n en constante desarrollo, pueden cambiar en cualquier momento y se pueden romper. Estan desactivadas por defecto y no se recomiendan para prod.</li> <li>V1.10.0-beta`: Son mas estables y no tienen tantos cambios. Ya han sido testeadas y s\u00ed est\u00e1n habilitadas por defecto y se consideran seguras para prod.</li> </ul> <p>El paquete de kubernetes que se descarga tiene todos los componentes de Kubernetes excepto ETCD Cluster y CoreDNS, ya que son proyectos separados</p> <p></p>"},{"location":"cluster-maintenance/#cluster-upgrade","title":"Cluster Upgrade","text":"<p>\u00bfEs obligatorio que todos los componentes de Kubernetes tengan las mismas versiones? No, los componentes pueden tener distintas versiones de lanzamiento.</p> <p>Nota: Kubernetes solo da soporte a las \u00faltimas 3 version</p> <p>Nota: Para hacer upgrades es recomendable hacerlo de una en una</p> <p>Existen 3 diferentes estrategias disponibles para hacer upgrade de los worker nodes:</p> <ol> <li>Hacer el upgrade en todos los nodos a la vez. Con desventaja de que las aplicaciones dejer\u00edan de funcionar</li> <li>Hacerlo nodo por nodo</li> <li>Agregar un nodo extra con la version nueva, mover las apps y matar el nodo viejito.</li> </ol>"},{"location":"cluster-maintenance/#kubeadm-upgrade","title":"Kubeadm upgrade","text":"<p>Asegurate que drenaste el nodo que vas a actualizar</p> <pre><code>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets\n</code></pre> <ol> <li>Primero se debe correr este para actualizar los repositorios de donde se va a descargar kubernetes</li> </ol> <pre><code>echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <pre><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <ol> <li>Despu\u00e9s hay que correr este, que te dir\u00e1 cuales version est\u00e1n disponibles</li> </ol> <pre><code>sudo apt-cache madison kubeadm\n</code></pre> <ol> <li>Actualizar el kubeadm</li> </ol> <pre><code>sudo apt-mark unhold kubeadm &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.30.x-*' &amp;&amp; \\\nsudo apt-mark hold kubeadm\n</code></pre> <p>Se puede verificar con <code>kubeadm version</code></p> <ol> <li>Correr el plan, este te dar\u00e1 el apply a ejecutar</li> </ol> <pre><code>kubeadm upgrade plan\n</code></pre> <ol> <li>Por ultimo correr el apply</li> </ol> <pre><code>kubeadm upgrade apply v1.30.x\n</code></pre> <p>Se puede verificar viendo este mensaje: <code>SUCCESS! Your cluster was upgraded to \"v1.30.x\". Enjoy!</code></p> <p>Nota: Se siguen los mismos pasos para los worker nodes, con diferencia que se corre <code>kubeadm upgrade node</code> en vez de <code>kubeadm upgradle apply</code></p> <ol> <li>Necesitas actualizar el kubelet y el kubectl, hay que correr el drain del nodo:</li> </ol> <p>Importante: Verifica la version de tus nodos, tendr\u00e1 la version viejita aun porque usa la version del kubelet con <code>kubectl get nodes</code></p> <p><pre><code># replace x in 1.30.x-* with the latest patch version\nsudo apt-mark unhold kubelet kubectl &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.30.x-*' kubectl='1.30.x-*' &amp;&amp; \\\nsudo apt-mark hold kubelet kubectl\n</code></pre> 7. Reinicia los servicios</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart kubelet\n</code></pre> <ol> <li>Por ultimo haz el uncordon</li> </ol> <pre><code>kubectl uncordon &lt;node-to-uncordon&gt;\n</code></pre> <p>Verifica que tus nodo se haya actualizado con <code>kubectl get nodes</code></p>"},{"location":"commands-and-arguments/","title":"Command & Arguments","text":"<p><code>--command -- &lt;comando&gt;</code>: Si deseas sobrecargar el comando que se ejecuta dentro del contenedor (por ejemplo, para ejecutar un proceso diferente a lo predeterminado).</p> <pre><code>kubectl run my-pod --image=nginx --command -- /bin/bash\n</code></pre> <pre><code>kubectl run my-pod --image=nginx --command -- sleep\n</code></pre> <p><code>--args</code>: Te permite especificar los argumentos que ser\u00e1n pasados al contenedor cuando se ejecute.</p> <pre><code>kubectl run my-pod --image=alpine --args=\u201c10\u201d\n</code></pre> <p>Nota: No se puede mandar ambos en el comando, puedes mandarlo con el command pero necesitas editar el pod para agregarle el arg, o viceversa.</p> <p>Nota 2: Se puede hacer con lista (este ejemplo es multi container):</p> <p></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu-sleeper-pod\nspec:\n containers:\n - name: ubuntu-sleeper\n   image: ubuntu-sleeper\n   command: [\"sleep2.0\"]\n   args: [\"10\"]\n</code></pre> <p></p>"},{"location":"containers/","title":"Containers","text":""},{"location":"containers/#multi-containers","title":"Multi Containers","text":"<p>Se usa cuando quieres correr mas de un container en el mismo Pod, no es tan com\u00fan.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp\n  labels:\n    name: simple-webapp\nspec:\n  containers:\n  - name: simple-webapp\n    image: simple-webapp\n    ports:\n    - containerPort: 8080\n  - name: log-agent\n    image: log-agent\n</code></pre> <p>En un pod multi-contenedor, se espera que cada contenedor ejecute un proceso que permanezca activo durante todo el ciclo de vida del POD. Por ejemplo, en el pod multi-contenedor del que hablamos anteriormente que tiene una aplicaci\u00f3n web y un agente de registro, se espera que ambos contenedores permanezcan activos en todo momento. Se espera que el proceso que se ejecuta en el contenedor del agente de registro permanezca activo mientras la aplicaci\u00f3n web est\u00e9 en funcionamiento. Si alguno de ellos falla, el POD se reinicia. Sin embargo, en ocasiones es posible que desees ejecutar un proceso que se complete en un contenedor. Por ejemplo, un proceso que extrae c\u00f3digo o un binario desde un repositorio que ser\u00e1 utilizado por la aplicaci\u00f3n web principal. Esa es una tarea que se ejecutar\u00e1 solo una vez cuando el pod se cree por primera vez. O un proceso que espera a que un servicio externo o una base de datos est\u00e9 activa antes de que la aplicaci\u00f3n real se inicie. Ah\u00ed es donde entran los initContainers. Un initContainer se configura en un pod como todos los dem\u00e1s contenedores, excepto que se especifica dentro de una secci\u00f3n initContainers, de esta manera:</p>"},{"location":"containers/#init-containers","title":"Init Containers","text":"<p>Se pueden agregar varios, corre de manera secuencial y si falla se reinicia hasta que sea satisfactorio.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>"},{"location":"environment-variables/","title":"Environment Variables","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\nspec:\n containers:\n - name: simple-webapp-color\n   image: simple-webapp-color\n   ports:\n   - containerPort: 8080\n   env:\n   - name: APP_COLOR\n     value: pink\n</code></pre> <p>There are 3 types of envs: </p> <p>Si solamente quieres una llave del configmap o secret como env en tu pod, se hace de esta forma: <pre><code>- name: UI_PROPERTIES_FILE_NAME\n  valueFrom:\n    configMapKeyRef:\n      name: game-demo\n      key: ui_properties_file_name\n</code></pre></p>"},{"location":"etcd/","title":"ETCD","text":""},{"location":"etcd/#backup-resource-configs","title":"Backup - Resource Configs","text":"<p>Este comando hace un backup de todos los recursos del cluster:</p> <pre><code>kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml (only for few resource groups)\n</code></pre>"},{"location":"etcd/#backend-restore-etcd","title":"Backend &amp; Restore - ETCD","text":"<ul> <li>Si el api-server es un service del sistema, primero haz el stop. Se puede revisar con  <code>systemctl status kube-apiserver</code></li> </ul> <pre><code>service kube-apiserver stop\n</code></pre> <ul> <li>La direcci\u00f3n local del etcd for defecto es: 127.0.0.1:2379</li> </ul>"},{"location":"etcd/#etcd-como-pod","title":"ETCD como pod","text":"<p>Para saber la informaci\u00f3n de etcd describe el pod:</p> <pre><code>kubectl describe pod -n kube-system etcd-controlplane\n</code></pre> <p>Esta informaci\u00f3n se usa en los comandos de <code>etcdtl</code></p> <ol> <li> <p>Haz el snapshot</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; \\\n  snapshot save &lt;backup-file-location&gt;\n</code></pre> </li> <li> <p>Para hacer el restore usa este comando</p> <pre><code>ETCDCTL_API=3 etcdctl --data-dir &lt;data-dir-location&gt; \\\n  snapshot restore &lt;backup-file-location&gt;\n</code></pre> <p>Si no funciona el comando as\u00ed, intenta pasandole las dem\u00e1s configuraciones.</p> </li> <li> <p>Actualiza el etcd manifest: /etc/kubernetes/manifests/etcd.yaml</p> <pre><code>volumes:\n- hostPath:\n  path: /var/lib/etcd-from-backup\n  type: DirectoryOrCreate\nname: etcd-data\n</code></pre> <p>Cuando se actualiza el manifest, el pod de ETCD automaticamente se va a recrear porque es un static pod.</p> <p>Nota: Aqu\u00ed se actualiza el volumen y no el <code>--data-dir</code> del command porque en el volumen es donde se hace referencia al restore del snashot en el host. Y el <code>--data-dir</code> es solo para dentro del container.</p> </li> </ol>"},{"location":"etcd/#etcd-como-external-service","title":"ETCD como external service","text":"<ol> <li> <p>Verifica si es un pod o un service</p> <p>Para revisar si es un pod: </p> <pre><code>kubectl get pods -n kube-system\n</code></pre> <p>Para revisar si un service:</p> <p><pre><code>systemctl status etcd\n</code></pre> Si no lo encuentras, verifica el api-server en el parametro de etcd, ah\u00ed podr\u00edas veritificar si es un external server.</p> </li> <li> <p>Haz el snapshot:</p> <p>Nota: Si el server de etcd se encuentra en el cluster, puedes hacer el snapshot desde ah\u00ed, si no, haz el ssh al server del etcd para que lo saques</p> <ul> <li>Y mueve el backup al nodo que se requiera, ya sea:<ul> <li>Desde el nodo1 copia el backup de etc-server: <code>scp etc-server:path/to/snashot /path/to/snashot</code></li> <li>Desde el etcd-server copia el backup al nodo 1: <code>scp /path/to/snashot nodo1:/path/to/snashot</code></li> </ul> </li> </ul> </li> <li> <p>Haz el restore</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem \\\n  --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db \\\n  --data-dir /var/lib/etcd-data-new\n</code></pre> </li> <li> <p>Actualiza el service</p> <p>Si no sabes el path del service sacalo con <code>sytemctl status etcd</code></p> <pre><code>vi /etc/systemd/system/etcd.service and add the new value for data-dir:\n</code></pre> </li> <li> <p>Actualiza los permisos del folder de data</p> <pre><code>chown -R etcd:etcd /var/lib/etcd-data-new\n\nls -ld /var/lib/etcd-data-new/\n</code></pre> </li> <li> <p>Reinicia los servicios</p> <pre><code>systemctl daemon-reload \netcd-server ~ \u279c  systemctl restart etcd\n</code></pre> <p>Y si el api-server es un service tambien reinicialo</p> <pre><code>service kube-apiserver start\n</code></pre> <p>Nota: Esto no viene en la documentaci\u00f3n oficial</p> </li> </ol>"},{"location":"networking/","title":"Networking","text":"<p>Este comando te ayuda a saber cual es el default gateway de un sistema:</p> <pre><code>ip route show default\n</code></pre> <p>Y para ver las interfaces de red: </p> <pre><code>ip link\n</code></pre> <p>O m\u00e1s especificamente:</p> <pre><code>controlplane ~ \u279c  ip link show cni0 \n5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1360 qdisc noqueue state UP mode DEFAULT group default qlen 1000\n    link/ether 5e:8c:d4:cc:64:d9 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>Para ver la internaz del bridge:</p> <pre><code>ip adress show type bridge\n</code></pre> <p>Para en general ver las rutas:</p> <pre><code>ip route\n</code></pre> <p>Para saber cuantas conexiones hay establecidas para un proceso de red:</p> <pre><code>netstate -npl | grep -i scheduler\n</code></pre> <p></p>"},{"location":"networking/#puertos-por-defecto-de-k8s","title":"Puertos por defecto de K8s","text":""},{"location":"networking/#cni","title":"CNI","text":"<p>Para checar los plugins de CNI soportados, se puede revisar:</p> <pre><code>ls /opt/cni/bin\n</code></pre> <p>Para checar que plugin usa el kubelet:</p> <pre><code>ls /etc/cni/net.d\n</code></pre>"},{"location":"networking/#que-maneja-kubeadm-y-que-no","title":"Que maneja Kubeadm y qu\u00e9 no?","text":"<p>Lo que maneja kubeadm autom\u00e1ticamente: Cuando configuras un cluster con kubeadm, este se encarga de gran parte de la configuraci\u00f3n de networking:</p> <ol> <li>Configuraci\u00f3n b\u00e1sica del plano de control: Establece los componentes principales como API Server, etcd, etc.</li> <li>Instalaci\u00f3n del CNI (Container Network Interface): kubeadm prepara el cluster para usar un plugin de red, pero necesitar\u00e1s instalarlo expl\u00edcitamente.</li> <li>Configuraci\u00f3n de kube-proxy: Este componente implementa las reglas de red para los servicios (incluyendo ClusterIP).</li> <li>Configuraci\u00f3n del DNS interno: Configura CoreDNS para la resoluci\u00f3n de nombres dentro del cluster.</li> </ol> <p>Lo que necesitas configurar manualmente:</p> <ol> <li> <p>Seleccionar e instalar un plugin CNI: Despu\u00e9s de inicializar el cluster con kubeadm, debes instalar un plugin de red como Calico, Flannel, Weave, etc. Esto es lo que realmente implementa la comunicaci\u00f3n pod-a-pod entre nodos.</p> <pre><code># Ejemplo con Calico\nkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre> </li> <li> <p>Configuraci\u00f3n de servicios: Los servicios como ClusterIP se crean mediante manifiestos de Kubernetes, no requieren configuraci\u00f3n manual de IP a bajo nivel.</p> </li> </ol> <p>Entonces, \u00bfnecesitas hacer ambos? No necesitas configurar manualmente las direcciones IP con comandos como <code>ip addr</code> en un cluster de producci\u00f3n. Las herramientas como kubeadm y los plugins CNI manejan toda la configuraci\u00f3n de red de bajo nivel. Lo que s\u00ed necesitas hacer es:</p> <ol> <li>Asegurarte de que tus nodos tengan conectividad de red entre s\u00ed</li> <li>Inicializar el cluster con kubeadm</li> <li>Instalar un plugin CNI</li> <li>Crear tus servicios (ClusterIP, etc.) mediante manifiestos de Kubernetes</li> </ol> <p>Los comandos como <code>ip addr</code> son \u00fatiles para entender y depurar c\u00f3mo funciona el networking internamente, pero no son parte del flujo normal de configuraci\u00f3n de un cluster Kubernetes.</p>"},{"location":"networking/#ips-por-defecto","title":"IPs por defecto","text":"<p>Las IPs privadas son 10.x.x.x, 172.x.x.x y 192.168.x.x</p> <p>Los CNI comunes usan los siguientes rangos de IP para los pods:</p> <p>Calico: Si no se configura un rango espec\u00edfico, Calico usa por defecto el rango 192.168.0.0/16 Flannel: Suele usar 10.244.0.0/16 por defecto Weave: Generalmente usa 10.32.0.0/12 por defecto, se puede cambiar con <code>weave launch --ipalloc-range=&lt;rango_de_ip&gt;</code></p>"},{"location":"networking/#comparativa","title":"Comparativa","text":""},{"location":"networking/#ingress","title":"Ingress","text":"<pre><code>kubectl create ingress ingress-test --rule=\u201cwear.my-online-store.com/wear*=wear-service:80\u201d\n</code></pre>"},{"location":"networking/#requerimientos","title":"Requerimientos","text":"<ul> <li>Ingress controller: Es un deployment, require poner unos labels y un configmap para tu nginx (si est\u00e1s usando nginx)</li> <li>Service: Se require que ese ingress-controller tenga un service, ya que este es el entrypoint desde el exterior</li> </ul>"},{"location":"networking/#ejemplos","title":"Ejemplos","text":"<p>Hace proxy de requests a <code>/pay</code> al backend <code>pay-service</code></p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n name: test-ingress\n namespace: critical-space\n annotations:\n nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n rules:\n - http:\n paths:\n - path: /pay\n backend:\n serviceName: pay-service\n servicePort: 8282\n</code></pre> <p>Hace proxy de requests que hacen match con la regex al backend</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n annotations:\n nginx.ingress.kubernetes.io/rewrite-target: /$2\n name: rewrite\n namespace: default\nspec:\n rules:\n - host: rewrite.bar.com\n http:\n paths:\n - backend:\n serviceName: http-svc\n servicePort: 80\n path: /something(/|$)(.*)\n</code></pre> <p>Nota Importante</p> <p>Existen anotaciones especificas por controller, por ejemplo para nginx esta anotaci\u00f3n es importante:</p> <pre><code>namespace: critical-space\nannotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n</code></pre> <p>Lo que hace es que al proxied service no le agrega el path que se haya configurado en el ingress. Por ejemplo, el usuario accede a: https://midominio.com/app/login</p> <ul> <li>En el ingress est\u00e1 configurado un path <code>/app</code></li> <li>Se hace un rewrite: <code>/app/login</code> --&gt; <code>/login</code></li> <li> <p>El backend tiene configurado una ruta de <code>/login</code> y responde correctamente:</p> <pre><code>app.get(\"/login\", (req, res) =&gt; {\n    res.send(\"Bienvenido al login!\");\n})\n</code></pre> </li> </ul>"},{"location":"networking/#gateway","title":"Gateway","text":"<p>El principal cambio del Ingress al Gateway es que el gateway tiene mas flexibilidad al momento de crear rutas y hacer el matching.</p> <p>El campo <code>allowedRoutes</code> determina a que  namespace se puede habilitar las HTTPRoute, TCPRoute, o otras rutas al Gateway. Configurando namespaces.from: All permite rutas desde todos los namespaces</p> <p>El Gateway API se enfoca principalmente en protocolos HTTP, HTTPS, TLS, TCP, y UDP para el routing. ICMP no est\u00e1 soportado</p>"},{"location":"networking/#gatewayclass","title":"GatewayClass","text":"<p>En el GatewayClass se define que controller (o servicio de proxy) se va a utilizar. Por ejemplo se podr\u00eda utilizar el nginx con gateway</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: example-class\nspec:\n  controllerName: example.com/gateway-controller\n</code></pre>"},{"location":"networking/#gateway_1","title":"Gateway","text":"<p>Define una instancia de infra que se encarga del manejo del trafico, como load balancer</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: nginx-gateway\nspec:\n  gatewayClassName: example-class\n  Allowed Routes:\n    namespaces: all\n  listeners:\n  - name: http\n    protocol: HTTP\n    port: 80\n</code></pre>"},{"location":"networking/#httproute","title":"HTTPRoute","text":"<p>Define reglas de mapeo de trafico a nivel de HTTP para el listener del Gateway a los endpoints de los servicios de backend.</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-httproute\nspec:\n  parentRefs:\n  - name: example-gateway\n  hostnames:\n  - \"www.example.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /login\n    backendRefs:\n    - name: example-svc\n      port: 8080\n</code></pre> <p> </p>"},{"location":"pod-affinity/","title":"Taints & Affinity","text":""},{"location":"pod-affinity/#taint","title":"Taint","text":"<p>Esto no garantiza que el pod decida a cual nodo irse, lo que hace es que el nodo pueda aceptar un pod o no </p> <p>There are 3 taint effects</p> <ul> <li>NoSchedule: ningun pod se va a ir a ese nodo a menos que tu hagas el toleration a el.</li> <li>PreferNoSchedule: intentara no ponerlo pero si no hay de otra pondra el pod ahi </li> <li>NoExecute: debes poner el toleration para que se qude ahi de otra forma mata todos </li> </ul> <p>To taint a node</p> <p><pre><code>kubectl taint nodes node1 app=blue:NoSchedule\n</code></pre> To remove a taint</p> <pre><code>kubectl taint nodes node1 key=values:NoSchedule-\n</code></pre>"},{"location":"pod-affinity/#selector-labels","title":"Selector &amp; Labels","text":"<p>El pod decide a cual node irse, es  como un label a un node</p> <pre><code>apiVersion:\nkind:\nmetadata:\n\nspec:\n  nodeSelector:\n    size: Large\n</code></pre> <p>Si algun objeto no define un <code>nodeSelector</code> entonces este se puede crear un cualquier nodo, a menos que tenga un taint.</p>"},{"location":"pod-affinity/#node-affinity","title":"Node Affinity","text":"<p>Igual para seleccionar un nodo, pero tiene m\u00e1s operadores: </p> <ul> <li>NotIn, </li> <li>In, </li> <li>Exists</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n name: myapp-pod\nspec:\n containers:\n - name: data-processor\n   image: data-processor\n affinity:\n   nodeAffinity:\n     requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: size\n            operator: In\n            values: \n            - Large\n            - Medium \n</code></pre>"},{"location":"pod-affinity/#node-affinity-types","title":"Node Affinity Types","text":"<ul> <li> <p>Available</p> <ul> <li>requiredDuringSchedulingIgnoredDuringExecution</li> <li>preferredDuringSchedulingIgnoredDuringExecution</li> </ul> </li> <li> <p>Planned</p> <ul> <li>requiredDuringSchedulingRequiredDuringExecution</li> <li>preferredDuringSchedulingRequiredDuringExecution</li> </ul> </li> </ul> <p></p>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/#montar-configmap-como-volumen","title":"Montar configmap como volumen","text":"<pre><code>  volumes:\n    - name: config-vol\n      configMap:\n        name: log-config\n        items:\n          - key: log_level\n            path: log_level.conf\n</code></pre>"},{"location":"storage/#montar-un-volume-con-mountpath","title":"Montar un volume con mountpath","text":"<pre><code>kind: Pod\nmetadata:\n  name: hostpath-example-linux\nspec:\n  os: { name: linux }\n  nodeSelector:\n    kubernetes.io/os: linux\n  containers:\n  - name: example-container\n    image: registry.k8s.io/test-webserver\n    volumeMounts:\n    - mountPath: /foo\n      name: example-volume\n      readOnly: true\n  volumes:\n  - name: example-volume\n    # mount /data/foo, but only if that directory already exists\n    hostPath:\n      path: /data/foo # directory location on host\n      type: Directory # this field is optional\n</code></pre>"},{"location":"storage/#persitent-volume","title":"Persitent Volume","text":"<p>La diferencia entre este y un volumen es que el volumen se va a perder al morirse el pod, adem\u00e1s que un persisten volume est\u00e1 ligado a un PVC</p> <pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n    name: pv-vol1\nspec:\n    accessModes: [ \"ReadWriteOnce\" ]\n    capacity:\n     storage: 1Gi\n    hostPath:\n     path: /tmp/data\n</code></pre> <p>El PV es un recurso m\u00e1s est\u00e1tico. Una vez creado, el PV es \u201cconcreto\u201d y est\u00e1  directamente asociado con un volumen f\u00edsico o l\u00f3gico.  Cualquier ajuste posterior (como cambiar el tipo de almacenamiento o las propiedades  del volumen) requiere modificar o reemplazar el PV.</p>"},{"location":"storage/#persistentvolumeclaim","title":"PersistentVolumeClaim","text":"<p>Se pueden ligar por medio de label o si no, k8s busca un PV para ligarse automaticamnete comparando sus recursos, y configuraciones</p> <p>Por ejemplo aqu\u00ed, se escoger\u00eda un PV que tenga al menos 1Gi y access mode \"ReadWriteOnce\"</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  resources:\n   requests:\n     storage: 1Gi\n</code></pre> <p>Y as\u00ed se usa este PVC en un pod</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: web\n  volumes:\n    - name: web\n      persistentVolumeClaim:\n        claimName: myclaim\n</code></pre>"},{"location":"storage/#storageclass","title":"StorageClass","text":"<p>Este te crear\u00e1 automaticamnete el PV, pero tu si debes crear tu PVC y ponerlo al pod</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: google-storage\nprovisioner: kubernetes.io/gce-pd\nparameters:\n    type: pd-standard [ pd-standard | pd-ssd ]\n    replication-type: none [ none | regional-pd ]\n</code></pre> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  storageClassName: google-storage       \n  resources:\n   requests:\n     storage: 500Mi\n</code></pre> <p>Al utilizar por ejemplo este provider the <code>gce-pd</code>, el cual es un Cloud Provider como de Google Cloud Platform, aun as\u00ed debemos crear a mano el disco en Cloud  para poder usarlo en k8s</p> <p>Tambien podemos definir un StorageClass que no crea el volumen para auto provisioning</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisioning\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"useful-commands/","title":"Useful commands","text":"<p>Correr pod sin usar yaml</p> <pre><code>kubectl run nginx --image=nginx\n</code></pre> <p>Crear un pod y agregarle un label</p> <pre><code>kubectl run redis --image=redis:alpine -l tier=db\n</code></pre> <p>Crear un service para exponer un pod</p> <pre><code>kubectl expose pod redis --port=6379 --name redis-service\n</code></pre> <p>Crear un deployment con 3 replicas</p> <pre><code>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3\n</code></pre> <p>Crear un pod y exponerlo en el puerto 8080 (solo exponer el pod, no crea un servicio)</p> <pre><code>kubectl run custom-nginx --image=nginx --port=8080\n</code></pre> <p>Crea un nuevo namespace</p> <pre><code>kubectl create ns dev-ns\n</code></pre> <p>Crear un deployment en un namespace con 2 replicas</p> <pre><code>kubectl create deployment redis-deploy -n dev-ns --image redis --replicas 2\n</code></pre> <p>Crea un pod y crea un service de tipo ClusterIP automaticamente</p> <pre><code>kubectl run httpd --image httpd:alpine --expose --port 80\n</code></pre> <p>Simula crear un deployment y te da el yaml</p> <pre><code>kubectl create deployent --image=nginx nginx --replicas=4 --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre> <p>Cambiarte de namespace</p> <pre><code>kubectl config set-context --current --namespace=&lt;namespace&gt;\n</code></pre> <p>Obtener el current namespace</p> <pre><code>kubectl config view | grep namespace\n</code></pre> <p>Actualizar un pod</p> <pre><code>kubectl replace --force -f nginx.yaml\n</code></pre> <p>cuenta cuantos pods, eliminando la columna <pre><code>kubectl get pod --selector env=prod --no-headers | wc -l\n</code></pre></p> <p>Aplicar un label a un nodo</p> <pre><code>kubectl label nodes node01 key=value\n</code></pre> <p>To apply a label to a node <pre><code>kubectl label nodes node-1 size=Large\n</code></pre></p> <p>Actualizar la imagen de un pod</p> <pre><code>kubectl set image pod/my-app my-container=nginx:latest\n</code></pre> <p>Te da el yaml de pod y le pasa un comando</p> <pre><code>kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 &gt; /etc/kubernetes/manifests/static-busybox.yaml\n</code></pre> <p>Obtener los eventos</p> <pre><code>kubectl get events -o wide\n</code></pre> <p>Escalar un deployment</p> <pre><code>kubectl scale deployment mydeployment --replicas=3\n</code></pre> <p>Ver los recursos de los pods</p> <pre><code>kubectl top pod -a\n</code></pre>"},{"location":"k8s-resources/configmap-and-secrets/","title":"Configmap & Secrets","text":""},{"location":"k8s-resources/configmap-and-secrets/#configmap","title":"ConfigMap","text":"<p>Para crear un configmap con llave-valor <pre><code>kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod\n</code></pre></p> <p>Para crear un configmap desde un archivo <pre><code>kubectl create configmap app-config --from-file=app_config.properties (Another way)\n</code></pre></p> <p></p> <p></p>"},{"location":"k8s-resources/configmap-and-secrets/#de-tipo-env","title":"De tipo env","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\nspec:\n containers:\n - name: simple-webapp-color\n   image: simple-webapp-color\n   ports:\n   - containerPort: 8080\n   envFrom:\n   - configMapRef:\n       name: app-config\n</code></pre>"},{"location":"k8s-resources/configmap-and-secrets/#de-single-env","title":"De single env","text":"<pre><code>- name: UI_PROPERTIES_FILE_NAME\n  valueFrom:\n    configMapKeyRef:\n      name: game-demo\n      key: ui_properties_file_name\n</code></pre>"},{"location":"k8s-resources/configmap-and-secrets/#de-tipo-volume","title":"De tipo volume","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n      readOnly: true\n  volumes:\n  - name: foo\n    configMap:\n      name: myconfigmap\n</code></pre>"},{"location":"k8s-resources/configmap-and-secrets/#secrets","title":"Secrets","text":"<p><pre><code>kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd\n</code></pre> <pre><code>kubectl create secret generic app-secret --from-file=app_secret.properties\n</code></pre></p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n name: app-secret\ndata:\n  DB_Host: bX1zcWw=\n  DB_User: cm9vdA==\n  DB_Password: cGFzd3Jk\n</code></pre> <p> </p>"},{"location":"k8s-resources/deployments/","title":"Deployments","text":"<p>Deployments crea autom\u00e1ticamente replicasets, que a su vez crea pods. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-deployment\n  labels:\n    app: myapp\n    type: front-end\nspec:\n template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n        type: front-end\n    spec:\n     containers:\n     - name: nginx-container\n       image: nginx\n replicas: 3\n selector:\n   matchLabels:\n    type: front-end\n</code></pre></p> <p>Solo aplicamos y listo!</p> <pre><code>kubectl create -f deployment-definition.yaml\n</code></pre> <p>O de forma imperativa <pre><code>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3\n</code></pre></p> <ul> <li>Recreate: Se bajan todos los pods y se suben todos</li> <li>RollingUpdate: Se baja uno y se se sube uno</li> </ul> <p></p>"},{"location":"k8s-resources/pods/","title":"Pods","text":""},{"location":"k8s-resources/pods/#definicion-en-yaml","title":"Definici\u00f3n en yaml","text":"<p>Se requieren forzosamente estos 4 valores: <pre><code>apiVersion:\nkind:\nmetadata:\n\nspec:\n</code></pre></p> <p>Si no hay un scheduler tu puedes asignarlo directamente en el yaml con: nodeName: o creando un archivo de tipo Binding y pasandolo por un http request</p>"},{"location":"k8s-resources/pods/#static-pods","title":"Static Pods","text":"<p>Por si quieres tener un nodo funcionando \"standalone\" sin un master node puedes configurar un directorio (/etc/kubernetes/manifests) donde colocas los yamls de los pods y el kubelet se encaragar\u00e1 de aplicarlos y actualizalos. Solo funciona con Pods, no funciona con Deployments, ni replica sets, etc</p> <p>Si tienes un cluster, tambien podr\u00edas ver los static pods pero no se pueden modificar, solo ver. Solo se pueden modificar/editar desde el nodo donde se cre\u00f3</p>"},{"location":"k8s-resources/replicasets-and-daemonset/","title":"ReplicaSets & DaemonSet","text":""},{"location":"k8s-resources/replicasets-and-daemonset/#replicaset","title":"ReplicaSet","text":""},{"location":"k8s-resources/replicasets-and-daemonset/#daemonset","title":"DaemonSet","text":"<p>Son como los replicasets, pero te ayudan a correr ultiples instancias de tu pod, pero corren siempre un pod en cada nodo.</p>"},{"location":"k8s-resources/serviceaccount/","title":"Service Accounts","text":"<p>A service account is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    kubernetes.io/enforce-mountable-secrets: \"true\"\n  name: my-serviceaccount\n  namespace: my-namespace\n</code></pre> <ul> <li>Por defecto hay un service account de default en cada namespace y se pone en todos los pods. Si no quieres poner el default puedes colocar la siguiente config</li> </ul> <pre><code>automountServiceAccountToken: false\n</code></pre> <p>Version 1.20: </p> <p>Al crear un serviceAccount te crea por defecto un secret con el token y se monta en los pods. El token no expira</p> <p>Version 1.22:</p> <p>Creas un service account que se monta en la direccion del token, este token expira</p> <p></p> <p>Version 1.24:</p> <p>Creas el service account pero tambien creas el secret, si es que quieres usar la antigua manera como la 20 pero en el secret tendras que ligar el service account</p> <p></p> <p>Para ver el token de un pod puedes ejecutar este comando</p> <pre><code>kubctl exec -it POD -- bash\ncat /var/run/secrets/kubernetes.io/token\n</code></pre>"},{"location":"k8s-resources/services/","title":"Services","text":""},{"location":"k8s-resources/services/#nodeport","title":"NodePort","text":"<ul> <li>Para acceder desde tu compu a un pod</li> <li>Rango v\u00e1lido para un nodeport: 30000 --&gt; 32767</li> <li>Solo es requerido <code>port</code>. El <code>targetPort</code> si no se pone, se usa el mismo que el port. <code>nodePort</code> si no se pone se usa uno randon dentro del rango.</li> <li>En un solo nodo: automatcamente se hace random load balancing a todos los pods que hagan match con el selector</li> <li>En varios nodos: se crea un service con el mismo <code>nodePort</code> en todos los nodos y se puede acceder con cualquier IP de cualquier nodo.</li> </ul>"},{"location":"k8s-resources/services/#clusterip","title":"ClusterIP","text":"<ul> <li>Para comunicaci\u00f3n dentro del cluster, no para acceder desde afuera</li> <li>Te agrupa tus pods en un service, al que se puede referir usando el service name.</li> </ul>"},{"location":"security/Authorization/","title":"Authorization","text":"<p>Hay diferentes mecanismos de autorizaci\u00f3n compatibles con Kubernetes</p> <ul> <li>Node Authorization</li> <li>Attribute-based Authorization (ABAC)</li> <li>Role-Based Authorization (RBAC)</li> <li>Webhook</li> </ul>"},{"location":"security/Authorization/#node-authorization","title":"Node Authorization","text":"<p>Se configura como parte del proceso de configuraci\u00f3n de kubelet para garantizar que el kubelet solo pueda hacer las cosas que est\u00e1n permitidas en el cl\u00faster.</p>"},{"location":"security/Authorization/#attribute-based-authorization-abac","title":"Attribute-based Authorization (ABAC)","text":"<p>Se configura mediante politicas, por ejemplo este define que el usuario alice, tiene acceso a pods en el namespace de frontend</p> <pre><code>{\n  \"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n  \"kind\": \"Policy\",\n  \"spec\": {\n    \"user\": \"alice\",\n    \"namespace\": \"frontend\",\n    \"resource\": \"pods\",\n    \"readonly\": true\n  }\n}\n</code></pre>"},{"location":"security/Authorization/#role-based-authorization-rbac","title":"Role-Based Authorization (RBAC)","text":"<p>Este tipo funciona medante 4 componentes principales:</p> <ol> <li> <p>Roles: Definen los permisos al nivel del namespace, especificando que acciones se pueden realizar sobre recursos espec\u00edfico (pod, servicios)</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> </li> <li> <p>ClusterRole: Similar a los roles pero con alcance al cluster completo, permitiendo gestionar recursos no namespaced como nodos.     <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-admin\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\", \"watch\", \"update\"]\n</code></pre></p> </li> <li> <p>RoleBindings: Conectan a los roles con los usuarios, grupos o service accounts dentro de un namespace     <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> </li> <li> <p>ClusterRoleBiding: Vinculan ClusterRoles con usuarios a nivel cluster     <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-admin-binding\nsubjects:\n- kind: User\n  name: admin\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: node-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> </li> </ol> <p>RBAC tambi\u00e9n se puede configurar imperativamente con los siguientes comandos:</p> <pre><code># Create a role named \"pod-reader\" that allows user to perform \"get\", \"watch\" and \"list\" on pods\nkubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods\n\n# Create a role named \"pod-reader\" with ResourceName specified\nkubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod\n\n# Create a role named \"foo\" with API Group specified\nkubectl create role foo --verb=get,list,watch --resource=rs.apps\n\n# Create a cluster role named \"pod-reader\" that allows user to perform \"get\", \"watch\" and \"list\" on pods\nkubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods\n\n# Create a cluster role named \"pod-reader\" with ResourceName specified\nkubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod\n\n# Create a cluster role named \"foo\" with API Group specified\nkubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps\n</code></pre> <p>Rolebindings:</p> <pre><code># Create a role binding for user1, user2, and group1 using the admin cluster role\nkubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1\n\n# Create a role binding for service account monitoring:sa-dev using the admin role\nkubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev\n\n# Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role\nkubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1\n</code></pre> <p>Nota: Se puede utilizar un ClusterRole y ligarlo a un RoleBinding, este patr\u00f3n tiene varias ventajas:</p> <ol> <li>Reutilizaci\u00f3n de roles: Puedes definir un conjunto de permisos una vez (en el ClusterRole) y reutilizarlo en m\u00faltiples namespaces</li> <li>Consistencia: Mantienes una definici\u00f3n \u00fanica de permisos</li> <li>Mantenibilidad: Si necesitas modificar los permisos, solo cambias el ClusterRole y se actualiza en todos los namespaces donde se usa</li> </ol>"},{"location":"security/Authorization/#webhook","title":"Webhook","text":"<p>Este modo funciona mediante un servicio HTTP externo, el cual deficide si las llamadas est\u00e1n autorizadas o no.</p> <p>Tu configuras un servicio y pones la url del server, este servicio debe responder con algo como</p> <pre><code>{\n  \"apiVersion\": \"authorization.k8s.io/v1\",\n  \"kind\": \"SubjectAccessReview\",\n  \"status\": {\n    \"allowed\": true,\n    \"reason\": \"user is authorized\"\n  }\n}\n</code></pre>"},{"location":"security/Role-clusterRole/","title":"RoleBinding vs ClusterRoleBinding (RBAC)","text":"<p>RoleBinding: Este se usa para asignar roles a usuarios o nodos dentro de un namespace espec\u00edfico. Es \u00fatil cuando deseas limitar el acceso a recursos dentro de un namespace espec\u00edfico.</p> <p>ClusterRoleBinding: Este se usa para asignar roles a nivel de cl\u00faster. Un ClusterRoleBinding puede otorgar permisos en todos los namespaces del cl\u00faster, no solo en un namespace espec\u00edfico.</p> <p></p> <p></p>"},{"location":"security/apiGroups/","title":"API Groups","text":"<p>La API de Kubernetes se agrupa en varios grupos seg\u00fan su finalidad, como uno para API, otro para Healthz, m\u00e9tricas y registros, etc.</p> <p>Estas API se clasifican en dos grupos: El grupo principal: donde se encuentra toda la funcionalidad</p> <p></p> <p>APIS </p> <p>To list all the api groups</p> <p></p> <p>P\u00e0ra evitar hacer el localhost para las api, puedes hacer un kubectl proxy y de esa forma evitas poner los certs(8081)</p> <p></p>"},{"location":"security/certificates/","title":"TLS Certificates","text":""},{"location":"security/certificates/#generar-certificados","title":"Generar Certificados","text":"<ul> <li>Hay varas tools, easyrsa, openssl or cfssl.</li> </ul>"},{"location":"security/certificates/#certificate-authority-ca","title":"Certificate Authority (CA)","text":"<ul> <li>Generar Keys   <pre><code>$ openssl genrsa -out ca.key 2048\n</code></pre></li> <li>Generar CSR   <pre><code>$ openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr\n</code></pre></li> <li>Firmar certificado   <pre><code>$ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt\n</code></pre></li> </ul>"},{"location":"security/certificates/#generar-certificados-de-cliente","title":"Generar Certificados de Cliente","text":""},{"location":"security/certificates/#admin-user-certificado","title":"Admin User Certificado","text":"<ul> <li>Generar Keys   <pre><code>$ openssl genrsa -out admin.key 2048\n</code></pre></li> <li>Generar CSR   <pre><code>$ openssl req -new -key admin.key -subj \"/CN=kube-admin\" -out admin.csr\n</code></pre></li> <li> <p>Firmar certificado   <pre><code>$ openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt\n</code></pre></p> </li> <li> <p>Certificado con permisos de admin   <pre><code>$ openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr\n</code></pre></p> </li> </ul>"},{"location":"security/certificates/#seguimos-el-mismo-procedimiento-para-generar-los-certificados-de-cliente-para-los-demas-components-que-acceden-al-kube-apiserver","title":"Seguimos el mismo procedimiento para generar los certificados de cliente para los dem\u00e1s components que acceden al kube-apiserver","text":""},{"location":"security/certificates/#generar-certificados-de-server","title":"Generar certificados de server","text":""},{"location":"security/certificates/#etcd-certificado-del-server","title":"ETCD Certificado del server","text":""},{"location":"security/certificates/#kube-apiserver-certificado","title":"Kube-apiserver Certificado","text":""},{"location":"security/certificates/#kubectl-nodes-server-cert","title":"Kubectl Nodes (Server Cert)","text":""},{"location":"security/certificates/#ver-certificados","title":"Ver Certificados","text":"<p>Para ver detalles de un cert</p> <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout\n</code></pre> <p></p> <p>Y para checar los logs puede ser de las siguientes maneras:</p> <ul> <li>Logs de un servicio</li> </ul> <pre><code>journalctl -u etcd.service -l\n</code></pre> <ul> <li>Logs de un pod</li> </ul> <pre><code>kubectl logs etcd-master\n</code></pre> <p>O si no puedes ver con kubectl puedes hacerlo con docker (o crictl) <pre><code>docker ps -a\ndocker logs &lt;container-id&gt;\n</code></pre></p>"},{"location":"security/certificates/#certificate-api","title":"Certificate API","text":""},{"location":"security/certificates/#este-certificado-se-puede-extraer-y-pasar-al-user","title":"Este certificado se puede extraer y pasar al user","text":"<ul> <li>El usuario crea su key   <pre><code>openssl genrsa -out jane.key 2048\n</code></pre></li> <li>Genera un CSR   <pre><code>openssl req -new -key jane.key -subj \"/CN=jane\" -out jane.csr \n</code></pre></li> <li>Envia la request y el admin toma esa key y crea un objeto de tipo CSR CertificateSigningRequest y el .csr del usuario   <pre><code>apiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata:\n  name: jane\nspec:\n  groups:\n  - system:authenticated\n  usages:\n  - digital signature\n  - key encipherment\n  - server auth\n  request:\n    &lt;certificate-goes-here&gt;\n</code></pre> <pre><code>cat jane.csr |base64 -w 0\nkubectl create -f jane.yaml\n</code></pre></li> </ul> <p><code>-w 0</code> sirve para quitar los saltos de linea del base64</p> <p></p> <ul> <li>Para ver los csr   <pre><code>kubectl get csr\n</code></pre></li> <li>Aprobar la request   <pre><code>kubectl certificate approve jane\n</code></pre></li> <li>Ver un certificado   <pre><code>kubectl get csr jane -o yaml\n</code></pre></li> <li>Para decodificarlo   <pre><code>echo \"&lt;certificate&gt;\" |base64 --decode\n</code></pre></li> </ul>"},{"location":"security/certificates/#todos-las-operaciones-de-certificados-las-hace-el-contoller-manager","title":"Todos las operaciones de certificados las hace el contoller manager","text":"<ul> <li>Si alguien tiene que firmar los certificados, deben tener los CA, root y private key del CA server. El controller manager ya tiene estas opciones</li> </ul>"},{"location":"security/image-security/","title":"Image security","text":"<p>Puedes usar registries publicos como Github, Dockerhub, etc o privados usando el <code>imagePullSecrets</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: private-registry.io/apps/internal-app\n  imagePullSecrets:\n  - name: regcred\n</code></pre> <p>Para crear el secret:</p> <pre><code>kubectl create secret docker-registry regcred \\\n  --docker-server=private-registry.io \\ \n  --docker-username=registry-user \\\n  --docker-password=registry-password \\\n  --docker-email=registry-user@org.com\n</code></pre> <p></p>"},{"location":"security/image-security/#security-context","title":"Security context","text":"<p>Para correr con un usuario espec\u00edfico:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n</code></pre> <p>o por container:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n    securityContext:\n      runAsUser: 1000\n</code></pre> <p>Para usar capabilities:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n    securityContext:\n      runAsUser: 1000\n      capabilities: \n        add: [\"MAC_ADMIN\"]\n</code></pre> <p></p>"},{"location":"security/kube-config/","title":"KubeConfig","text":"<p><pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://controlplane:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: DATA+OMITTED\n    client-key-data: DATA+OMITTED\n</code></pre> Nota certificate-authority-data: le pones el ca.crt  Aqui tienes que hacerlo en base64  <pre><code>cat ca.crt | base64 \n</code></pre></p> <p>certificate-authority: en donde esta ubicado el crt</p> <p>El cliente usa el archivo de certificado y la clave para consultar la API Rest de Kubernetes y obtener una lista de pods que usan curl.</p> <p></p> <p>Podemos trasladar esta informaci\u00f3n a un archivo de configuraci\u00f3n llamado kubeconfig y especificar este archivo como la opci\u00f3n kubeconfig en el comando.</p> <pre><code>$ kubectl get pods --kubeconfig config\n</code></pre> <p>Kubeconfig File The kubeconfig tien 3 secciones</p> <ul> <li>Clusters</li> <li>Contexts</li> <li>USers</li> </ul> <p>Cluster son los cluster que est\u00e1n disponibles, user es quien esta permitido y el context es la combinaciondel cluster con el user, de esa forma se evita en meter los certicados en cada petici\u00f3n que uno haga</p> <p>Puede especificar el archivo kubeconfig con la vista de configuraci\u00f3n de kubectl con el indicador \"--kubeconfig\"</p> <pre><code>$ kubectl config veiw --kubeconfig=my-custom-config\n</code></pre> <p>\u00bfC\u00f3mo se actualiza el current-context? \u00bfO se cambia el current-context?</p> <pre><code>$ kubectl config use-context &lt;context-name&gt;\nex: \n$ kubectl config use-context prod-user@production\n</code></pre> <p>What about namespaces? Especificar dentro del context el namespace que quieras </p> <pre><code>apiVersion: v1\nkind: Config\n\nclusters:\n- name: production\n  cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n</code></pre> <p>Para cambiar el kubeconfig sin necesidad de usar el indicador \"--kubeconfig\" </p> <ol> <li>/root/.kube/config o ~/.kube/config sobre escribir el kubeconfig</li> <li>sin sobrescribir creas una variable en \"/root/.bashrc <pre><code>export KUBECONFIG=/kubeconfig\nsource ~/.bashrc\n</code></pre></li> </ol>"},{"location":"security/network-policies/","title":"Network Policies","text":"<p>Hay dos tipos:</p> <ul> <li>Ingress</li> <li>Egress</li> </ul> <p>Para crear un network policy:</p> <p>Se agrega el label del pod que quieres relacionar: </p> <pre><code>spec:\n  podSelector:\n    matchLabels:\n</code></pre> <p>Luego agregas el <code>from</code> cuando es ingress e igual pones el label del pod al que le vas a permitir que se comunique contigo/obtenga info de ti y agregar el puerto</p> <pre><code>ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api-pod\n</code></pre> <p>Ejemplo de ingress</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n name: db-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api-pod\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <p>Puedes permitir por IP y bloquear las que no</p> <pre><code>- ipBlock:\n        cidr: 172.17.0.0/16\n        except:\n        - 172.17.1.0/24\n</code></pre> <p>Igual puedes permitir solo de namespaces specificos</p> <pre><code> - namespaceSelector:\n        matchLabels:\n          project: myproject\n</code></pre> <p>Ejemplo de ingress y egress</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 172.17.0.0/16\n        except:\n        - 172.17.1.0/24\n    - namespaceSelector:\n        matchLabels:\n          project: myproject\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 6379\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/24\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 5978\n</code></pre> <p>Y para egress igual, si te piden dos pods diferentes pones dos <code>to</code></p> <pre><code>egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/24\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 5978\n\n  - to:\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 5978\n</code></pre>"}]}